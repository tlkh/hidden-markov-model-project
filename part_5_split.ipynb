{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import emission\n",
    "import transition\n",
    "import viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"data/AL/\"\n",
    "train_data = dataset_folder + \"train\"\n",
    "lines = utils.read_file_to_lines(train_data)\n",
    "\n",
    "entity_lines = []\n",
    "sentiment_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        word, tag = line.split(\" \")\n",
    "        entity, sentiment = tag.split(\"-\")\n",
    "        entity_lines.append(word + \" \" + entity)\n",
    "        sentiment_lines.append(word + \" \" + sentiment)\n",
    "    except:\n",
    "        entity_lines.append(line)\n",
    "        sentiment_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset_folder + \"dev.in\"\n",
    "lines = utils.read_file_to_lines(test_data)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "while len(lines) > 1:\n",
    "    sentence_break = lines.index(\"\")\n",
    "    sentence_xy = lines[:sentence_break]\n",
    "    words = [token.strip() for token in sentence_xy]\n",
    "    sentence = \" \".join(words).strip()\n",
    "    sentences.append(sentence)\n",
    "    lines = lines[sentence_break+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lines(lines):\n",
    "    emission_data = emission.generate_emission_table(lines)\n",
    "    hashmap = emission_data[\"x_hashmap\"]\n",
    "    word_freq = emission_data[\"x_word_freq\"]\n",
    "    smoothed_hashmap = utils.add_unk(hashmap, word_freq, k=3)\n",
    "    emission_data[\"x_hashmap\"] = smoothed_hashmap\n",
    "\n",
    "    x_vocab = utils.get_emission_vocab(smoothed_hashmap)\n",
    "\n",
    "    transition_pairs = transition.generate_transition_pairs(lines)\n",
    "\n",
    "    y_pairs = transition_pairs[\"Y_pairs\"]\n",
    "    y_vocab = transition_pairs[\"y_vocab\"]\n",
    "    y_freq = transition_pairs[\"y_freq\"]\n",
    "\n",
    "    transition_data = transition.generate_transition_data(y_pairs, y_vocab)\n",
    "\n",
    "    hmm = viterbi.HMM()\n",
    "\n",
    "    hmm.fit_word_tokenizer(x_vocab)\n",
    "    hmm.fit_pos_tokenizer(y_vocab)\n",
    "\n",
    "    hmm.build_transition_weights(y_freq, transition_data)\n",
    "    hmm.build_emission_weights(emission_data)\n",
    "    \n",
    "    # only for the progress bar!\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        USE_TQDM = True\n",
    "    except Exception as e:\n",
    "        print(e, \"TQDM import error, disable progress bar\")\n",
    "\n",
    "    if USE_TQDM:\n",
    "        sentences_it = tqdm(sentences)\n",
    "    else:\n",
    "        sentences_it = sentences\n",
    "        \n",
    "    preds = []\n",
    "\n",
    "    for line in sentences_it:\n",
    "        pred = hmm.viterbi_predict(line)\n",
    "        pred = hmm.pos_tokens_to_labels(pred)\n",
    "        preds.append(pred)\n",
    "\n",
    "    assert len(sentences) == len(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1492/1492 [00:01<00:00, 1337.64it/s]\n",
      "100%|██████████| 1492/1492 [00:33<00:00, 45.19it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_preds = predict_lines(entity_lines)\n",
    "sentiment_preds = predict_lines(sentiment_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = dataset_folder + \"dev.p5.out\"\n",
    "\n",
    "with open(outfile, \"w\") as f:\n",
    "    for sentence, entity_pred, sentiment_pred in zip(sentences, entity_preds, sentiment_preds):\n",
    "        word_array = sentence.split(\" \")\n",
    "        try:\n",
    "            for i, word in enumerate(word_array):\n",
    "                if entity_pred[i] == \"O\" or sentiment_pred[i] == \"O\":\n",
    "                    f.write(word + \" O\\n\")\n",
    "                else:\n",
    "                    f.write(word + \" \" + entity_pred[i] + \"-\" + sentiment_pred[i] +\"\\n\")\n",
    "        except:\n",
    "            print(word_array)\n",
    "            print(pred)\n",
    "            break\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity F: 0.5097\n",
      "Entity precision: 0.4392\n",
      "Entity recall: 0.607\n",
      "Sentiment F: 0.4436\n",
      "Sentiment precision: 0.3823\n",
      "Sentiment recall: 0.5283\n"
     ]
    }
   ],
   "source": [
    "gold_data = dataset_folder + \"dev.out\"\n",
    "pred_data = dataset_folder + \"dev.p5.out\"\n",
    "\n",
    "data = utils.run_eval(gold_data, pred_data)\n",
    "\n",
    "print(\"Entity F:\", data[\"entity_f\"])\n",
    "print(\"Entity precision:\", data[\"entity_p\"])\n",
    "print(\"Entity recall:\", data[\"entity_r\"])\n",
    "print(\"Sentiment F:\", data[\"sentiment_f\"])\n",
    "print(\"Sentiment precision:\", data[\"sentiment_p\"])\n",
    "print(\"Sentiment recall:\", data[\"sentiment_r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
