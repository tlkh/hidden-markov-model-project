{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import emission\n",
    "import transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER = True\n",
    "NORM_TENSE = True\n",
    "REP_NUM = True\n",
    "REP_YEAR = True\n",
    "REP_SYM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"data/AL/\"\n",
    "train_data = dataset_folder + \"train\"\n",
    "lines = utils.read_file_to_lines(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to unpack (expected 2, got 1)\n",
      "Skipped 1 lines:  ['']\n",
      "Vocab size: 1078\n"
     ]
    }
   ],
   "source": [
    "emission_data = emission.generate_emission_table(lines,\n",
    "                                                 lower=LOWER,\n",
    "                                                 norm_tense=NORM_TENSE,\n",
    "                                                 replace_number=REP_NUM,\n",
    "                                                 replace_year=REP_YEAR,\n",
    "                                                 replace_symbol=REP_SYM)\n",
    "hashmap = emission_data[\"x_hashmap\"]\n",
    "word_freq = emission_data[\"x_word_freq\"]\n",
    "smoothed_hashmap = utils.add_unk(hashmap, word_freq, k=4)\n",
    "emission_data[\"x_hashmap\"] = smoothed_hashmap\n",
    "\n",
    "x_vocab = utils.get_emission_vocab(smoothed_hashmap)\n",
    "print(\"Vocab size:\", len(x_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_pairs = transition.generate_transition_pairs(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pairs = transition_pairs[\"Y_pairs\"]\n",
    "y_vocab = transition_pairs[\"y_vocab\"]\n",
    "y_freq = transition_pairs[\"y_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_data = transition.generate_transition_data(y_pairs, y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = viterbi.HMM()\n",
    "hmm.fit_word_tokenizer(x_vocab)\n",
    "hmm.fit_pos_tokenizer(y_vocab)\n",
    "hmm.build_transition_weights(y_freq, transition_data)\n",
    "hmm.build_emission_weights(emission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_folder + \"dev.in\"\n",
    "lines = utils.read_file_to_lines(train_data)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "while len(lines) > 1:\n",
    "    sentence_break = lines.index(\"\")\n",
    "    sentence_xy = lines[:sentence_break]\n",
    "    words = [utils.preprocess_text(token,\n",
    "                                   lower=LOWER,\n",
    "                                   norm_tense=NORM_TENSE,\n",
    "                                   replace_number=REP_NUM,\n",
    "                                   replace_year=REP_YEAR,\n",
    "                                   replace_symbol=REP_SYM)\n",
    "             for token in sentence_xy]\n",
    "    sentence = \" \".join(words).strip()\n",
    "    sentences.append(sentence)\n",
    "    lines = lines[sentence_break+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['娴 姹 鐪 鏉 宸 甯 涓 鍩 鍖 鍗 鏄 琛 閬 澶 鍏 璺 濂 鏈 瑙 5 骞',\n",
       " '涓 鍗 璺 216 鍙 14 鍙 妤 鍗 7 妤',\n",
       " '娴 姹 鐪 姹 灞 甯 鍙 濉 琛 閬 闄 鏉 鏉 闄 鏉 309 鍙',\n",
       " '鏉 宸 鎷 澧 鍖 鏂 瀹 鑺 鑻 135 鏍 7 - 1121',\n",
       " '娴 姹 鐪 瀹 娉 甯 浣 濮 甯 鍏 姹 琛 閬 涓 鍑 妗 鏉',\n",
       " '婀 闃 闀 娌 鍗 鐪 鍞 娌 鍘 婀 闃 闀 婀 闃 鏈 琛',\n",
       " '涓 娌 鍥 鍙 澶 琛 鍖 閾 鍏 瀵 16 骞 2299 瀹',\n",
       " '娴 姹 鐪 缁 鍏 甯 宓 宸 甯 宕 浠 闀 婀 鏉 妗 鏉',\n",
       " '闃 鏄 琛 閬 鏃 灞 鏉 鐐 鍏 灞 392 鍙',\n",
       " '瀹 娉 姹 涓 鍖 涓 灞 璺 608 鍙 閾 娉 鐧 璐 9 妤 澶 骞 楦 鐢 瑁']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1492 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# only for the progress bar!\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    USE_TQDM = True\n",
    "except Exception as e:\n",
    "    print(e, \"TQDM import error, disable progress bar\")\n",
    "\n",
    "if USE_TQDM:\n",
    "    sentences_it = tqdm(sentences)\n",
    "else:\n",
    "    sentences_it = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1492/1492 [01:57<00:00, 12.68it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for line in sentences_it:\n",
    "    pred = hmm.viterbi_predict(line)\n",
    "    pred = hmm.pos_tokens_to_labels(pred)\n",
    "    preds.append(pred)\n",
    "    \n",
    "assert len(sentences) == len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = dataset_folder + \"dev.p5.out\"\n",
    "\n",
    "with open(outfile, \"w\") as f:\n",
    "    for sentence, pred in zip(sentences, preds):\n",
    "        word_array = sentence.split(\" \")\n",
    "        try:\n",
    "            assert len(word_array) == len(pred)\n",
    "            for i, word in enumerate(word_array):\n",
    "                f.write(word + \" \" + pred[i] +\"\\n\")\n",
    "        except:\n",
    "            print(word_array)\n",
    "            print(pred)\n",
    "            break\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 8408\n",
      "#Entity in prediction: 9213\n",
      "\n",
      "#Correct Entity : 5308\n",
      "Entity  precision: 0.5761\n",
      "Entity  recall: 0.6313\n",
      "Entity  F: 0.6025\n",
      "\n",
      "#Correct Sentiment : 4562\n",
      "Sentiment  precision: 0.4952\n",
      "Sentiment  recall: 0.5426\n",
      "Sentiment  F: 0.5178\n"
     ]
    }
   ],
   "source": [
    "!python3 evalResult.py ./data/AL/dev.out ./data/AL/dev.p5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
